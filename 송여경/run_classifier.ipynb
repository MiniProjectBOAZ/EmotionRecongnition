{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XeLVPguP1b_","executionInfo":{"status":"ok","timestamp":1724294116600,"user_tz":-540,"elapsed":67876,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"d7e0a8e7-256c-4ab9-89d8-2078b0cd7e0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.3.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.26.4)\n","Collecting boto3 (from pytorch_pretrained_bert)\n","  Downloading boto3-1.35.3-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2024.5.15)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch_pretrained_bert)\n","  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting botocore<1.36.0,>=1.35.3 (from boto3->pytorch_pretrained_bert)\n","  Downloading botocore-1.35.3-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_pretrained_bert)\n","  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2024.7.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.3->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.3->boto3->pytorch_pretrained_bert) (1.16.0)\n","Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading boto3-1.35.3-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m274.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.35.3-py3-none-any.whl (12.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_pretrained_bert\n","Successfully installed boto3-1.35.3 botocore-1.35.3 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pytorch_pretrained_bert-0.6.2 s3transfer-0.10.2\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"]}],"source":["!pip install pytorch_pretrained_bert\n","!pip install transformers\n","\n","from __future__ import absolute_import, division, print_function\n","import argparse\n","import os\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","from tqdm import tqdm, trange\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE,cached_path\n","from model import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n","from transformers import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert.optimization import BertAdam\n","from utils import *"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Google Drive에서 파일을 복사\n","!cp /content/drive/MyDrive/송여경/model.py .\n","!cp /content/drive/MyDrive/송여경/utils.py ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xzh6wEjRvGR","executionInfo":{"status":"ok","timestamp":1724294036421,"user_tz":-540,"elapsed":46036,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"fb8de204-ce4d-4466-f4e0-7c5317970934"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/')\n","\n","!cp /content/drive/MyDrive/main.py .\n","!cp /content/drive/MyDrive/utils.py .\n","!cp /content/drive/MyDrive/model.py .\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CZ87vMoRDw7","executionInfo":{"status":"ok","timestamp":1724294124788,"user_tz":-540,"elapsed":523,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"33797dfa-63c8-45d5-dd7a-54ed7c169369"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/drive/MyDrive/main.py': No such file or directory\n","cp: cannot stat '/content/drive/MyDrive/utils.py': No such file or directory\n","cp: cannot stat '/content/drive/MyDrive/model.py': No such file or directory\n","drive  model.py  __pycache__  sample_data  utils.py\n"]}]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n","!pip show kobert_tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBZqG57dP9dw","executionInfo":{"status":"ok","timestamp":1724294143580,"user_tz":-540,"elapsed":14495,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"1e9918c3-6c7e-406f-f5f5-b01ffc4cf5e9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-ztty5a0g/kobert-tokenizer_832f03da8b2d48af92191382a76c94d3\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-ztty5a0g/kobert-tokenizer_832f03da8b2d48af92191382a76c94d3\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4632 sha256=04f33d19b676e92358ca0b074fdb47362f74d56bbd25ea8117f354b9166573b9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-zmff6n1x/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n","Name: kobert-tokenizer\n","Version: 0.1\n","Summary: Korean BERT pre-trained cased (KoBERT) for HuggingFace \n","Home-page: https://github.com/SKTBrain/KoBERT\n","Author: SeungHwan Jung\n","Author-email: digit82@gmail.com\n","License: Apache-2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: \n","Required-by: \n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Hugging Face Transformers의 KoBERT\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNKAZ6Kw_6-T","executionInfo":{"status":"ok","timestamp":1724295135863,"user_tz":-540,"elapsed":735,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"66ba5e91-701e-493e-a4c7-e6099a5c0b3b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, accuracy_score\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"],"metadata":{"id":"v2BPDd8uQA8m","executionInfo":{"status":"ok","timestamp":1724294150907,"user_tz":-540,"elapsed":4449,"user":{"displayName":"송여경","userId":"16440594923547462791"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-_jdWoKEAeZ","executionInfo":{"status":"ok","timestamp":1724296245692,"user_tz":-540,"elapsed":33498,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"a58535c8-c478-4aae-bfa0-5ab0790e5b03"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.26.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.32.3)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.7.4)\n","Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.3\n","    Uninstalling graphviz-0.20.3:\n","      Successfully uninstalled graphviz-0.20.3\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.26.4)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.11)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (24.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp310-cp310-linux_x86_64.whl size=661654 sha256=f84d3ddc8142dc7ef4f82ef23000cfc2ffe692ff1d4b43c5a754a62d91d3d6f6\n","  Stored in directory: /root/.cache/pip/wheels/1a/1e/0d/99f55911d90f2b95b9f7c176d5813ef3622894a4b30fde6bd3\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1EfLypHECON","executionInfo":{"status":"ok","timestamp":1724296260468,"user_tz":-540,"elapsed":8951,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"1c0e6e63-fa7b-45e4-cec8-35cde7d3818a"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-20xhe4bv/kobert-tokenizer_936e6ad19b9742ef8a207a6ae92d92f4\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-20xhe4bv/kobert-tokenizer_936e6ad19b9742ef8a207a6ae92d92f4\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["pip install numpy==1.19.5\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWVBJBh6FyXZ","executionInfo":{"status":"ok","timestamp":1724296828057,"user_tz":-540,"elapsed":155939,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"05c55474-a986-42e7-ad9e-91063209442e"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.19.5\n","  Downloading numpy-1.19.5.zip (7.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: numpy\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for numpy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\u001b[31m\n","\u001b[0mFailed to build numpy\n","\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (numpy)\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","device = torch.device(\"cuda:0\")\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"8n0qg9NHEHee","executionInfo":{"status":"error","timestamp":1724296621604,"user_tz":-540,"elapsed":581,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"8f16ecaf-1e50-44cc-cf62-3ca9dd5d976e"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/mxnet/numpy/utils.py:37: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n","  bool = onp.bool\n"]},{"output_type":"error","ename":"AttributeError","evalue":"module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-4d475aad730e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgluonnlp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gluonnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/contrib/text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/contrib/text/embedding.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_np_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_mx_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy_extension\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_mx_npx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmultiarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_register\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/numpy/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_internal\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_npi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_storage_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_np_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import,unused-wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/numpy/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mint64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mbool_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__former_attrs__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__former_attrs__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'testing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from kobert_tokenizer import KoBERTTokenizer\n","import torch\n","import argparse\n","\n","def main(i):\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument(\"--data_dir\", default='data/', type=str)\n","    parser.add_argument(\"--bert_model\", default='monologg/kobert', type=str)\n","    parser.add_argument(\"--task_name\", default='Multi', type=str)\n","    parser.add_argument(\"--output_dir\", default='CM-BERT_output/', type=str)\n","    parser.add_argument(\"--cache_dir\", default=\"\", type=str)\n","    parser.add_argument(\"--max_seq_length\", default=100, type=int)\n","    parser.add_argument(\"--do_train\", action='store_true')\n","    parser.add_argument(\"--do_test\", action='store_true')\n","    parser.add_argument(\"--do_lower_case\", action='store_true')\n","    parser.add_argument(\"--train_batch_size\", default=24, type=int)\n","    parser.add_argument(\"--eval_batch_size\", default=24, type=int)\n","    parser.add_argument(\"--test_batch_size\", default=24, type=int)\n","    parser.add_argument(\"--learning_rate\", default=2e-5, type=float)\n","    parser.add_argument(\"--num_train_epochs\", default=3.0, type=float)\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float)\n","    parser.add_argument(\"--no_cuda\", action='store_true')\n","    parser.add_argument('--seed', type=int, default=11111)\n","    parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n","\n","    args = parser.parse_args(args=[\"--do_train\", \"--do_test\"])\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","\n","    # KoBERTTokenizer 초기화\n","    vocab_file = '/content/drive/MyDrive/송여경/spiece.model'  # 실제 KoBERT spiece.model 파일 경로로 변경해야 합니다.\n","    tokenizer = KoBERTTokenizer(vocab_file=vocab_file)\n","\n","    # KoBERT 모델 로드\n","    model = BertForSequenceClassification.from_pretrained(args.bert_model, num_labels=7)\n","\n","    model.to(device)\n","\n","    processors = {\n","        \"multi\": PgProcessor,\n","    }\n","    num_labels_task = {\n","        \"multi\": 7,\n","    }\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","    n_gpu = torch.cuda.device_count()  # GPU의 수를 가져옵니다.\n","    logger.info(\"device: {} n_gpu: {}\".format(device, n_gpu))\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(args.gradient_accumulation_steps))\n","\n","    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n","\n","    seed_num = np.random.randint(1, 10000)\n","    random.seed(seed_num)\n","    np.random.seed(seed_num)\n","    torch.manual_seed(seed_num)\n","    if n_gpu > 0:\n","        torch.cuda.manual_seed_all(seed_num)\n","\n","    if not args.do_train and not args.do_test:\n","        raise ValueError(\"At least one of `do_train` or `do_test` must be True.\")\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    task_name = args.task_name.lower()\n","\n","    if task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (task_name))\n","\n","    processor = processors[task_name]()\n","    num_labels = num_labels_task[task_name]\n","    label_list = processor.get_labels()\n","\n","    tokenizer = KoBERTTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) ### KoBERT\n","\n","    train_examples = None\n","    num_train_optimization_steps = None\n","    if args.do_train:\n","        train_examples = processor.get_train_examples(args.data_dir)\n","        num_train_optimization_steps = int(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n","\n","    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(\"-1\"))\n","\n","    model = BertForSequenceClassification.from_pretrained(args.bert_model, cache_dir=cache_dir, num_labels=num_labels)\n","\n","    for name, param in model.named_parameters():\n","        param.requires_grad = False\n","        if \"encoder.layer.0\" in name or \"encoder.layer.1\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.2\" in name or \"encoder.layer.3\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.4\" in name or \"encoder.layer.5\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.6\" in name or \"encoder.layer.7\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.8\" in name or \"encoder.layer.9\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n","            param.requires_grad = True\n","        if \"BertFinetun\" in name or \"pooler\" in name:\n","            param.requires_grad = True\n","\n","    model.to(device)\n","\n","    if n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    param_optimizer = list(model.named_parameters())\n","\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    new_decay = ['BertFine']\n","\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and not any(np in n for np in new_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and any(np in n for np in new_decay)], 'lr': 0.01}\n","    ]\n","\n","    optimizer = BertAdam(optimizer_grouped_parameters,\n","                         lr=args.learning_rate,\n","                         warmup=args.warmup_proportion,\n","                         t_total=num_train_optimization_steps)\n","\n","    global_step = 0\n","    nb_tr_steps = 0\n","    tr_loss = 0\n","\n","    audio_npy = np.load('/content/drive/MyDrive/송여경/audio-001.npy')\n","    train_audio, test_audio = audio_npy[:8231], audio_npy[8231:]\n","\n","    if args.do_train:\n","\n","        train_features = convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer)\n","        logger.info(\"***** Running training *****\")\n","        logger.info(\"  Num examples = %d\", len(train_examples))\n","        logger.info(\"  Batch size = %d\", args.train_batch_size)\n","        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n","        all_train_audio = torch.tensor(train_audio, dtype=torch.float32)\n","        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float32)\n","\n","        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_train_audio, all_label_ids)\n","        train_sampler = RandomSampler(train_data)\n","        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n","            model.train()\n","            tr_loss = 0\n","            nb_tr_examples, nb_tr_steps = 0, 0\n","            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n","                batch = tuple(t.to(device) for t in batch)\n","                input_ids, input_mask, segment_ids, all_train_audio, label_ids = batch\n","                loss = model(input_ids, all_train_audio, segment_ids, input_mask, label_ids)\n","                if n_gpu > 1:\n","                    loss = loss.mean()\n","                if args.gradient_accumulation_steps > 1:\n","                    loss = loss / args.gradient_accumulation_steps\n","\n","                loss.backward()\n","\n","                tr_loss += loss.item()\n","                nb_tr_examples += input_ids.size(0)\n","                nb_tr_steps += 1\n","                if (step + 1) % args.gradient_accumulation_steps == 0:\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    global_step += 1\n","\n","    if args.do_test:\n","\n","        test_examples = processor.get_test_examples(args.data_dir)\n","        test_features = convert_examples_to_features(test_examples, label_list, args.max_seq_length, tokenizer)\n","        logger.info(\"\")\n","        logger.info(\"***** Running test *****\")\n","        logger.info(\"  Num examples = %d\", len(test_examples))\n","        logger.info(\"  Batch size = %d\", args.test_batch_size)\n","\n","        all_test_audio = torch.tensor(test_audio, dtype=torch.float32, requires_grad=True)\n","        all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n","        all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n","        all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n","        all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.float32)\n","\n","        test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_test_audio)\n","\n","        test_sampler = SequentialSampler(test_data)\n","        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.test_batch_size)\n","\n","        model.eval()\n","\n","        test_loss, test_accuracy = 0, 0\n","        nb_test_steps, nb_test_examples = 0, 0\n","        predict_list = []\n","        truth_list = []\n","        text_attention_list = []\n","        fusion_attention_list = []\n","\n","        logits_list = []\n","\n","        with torch.no_grad():\n","            for input_ids, input_mask, segment_ids, label_ids, all_test_audio in tqdm(test_dataloader, desc=\"Evaluating\"):\n","                input_ids = input_ids.to(device)\n","                input_mask = input_mask.to(device)\n","                segment_ids = segment_ids.to(device)\n","                label_ids = label_ids.to(device)\n","                all_test_audio = all_test_audio.to(device)\n","\n","                with torch.no_grad():\n","                    tmp_test_loss = model(input_ids, all_test_audio, segment_ids, input_mask, label_ids)\n","                    logits, text_attention, fusion_attention = model(input_ids, all_test_audio, segment_ids, input_mask)\n","\n","                logits_tmp = logits.detach().cpu().numpy()\n","\n","                logits_list.append(logits_tmp)\n","\n","                logits = torch.argmax(logits, dim=1)\n","                logits = logits.detach().cpu().numpy()\n","                label_ids = label_ids.to('cpu').numpy()\n","                text_attention = text_attention.cpu().numpy()\n","                fusion_attention = fusion_attention.cpu().numpy()\n","                test_loss += tmp_test_loss.mean().item()\n","\n","                for i in range(len(logits)):\n","                    predict_list.append(logits[i])\n","                    truth_list.append(label_ids[i])\n","                    text_attention_list.append(text_attention[i])\n","                    fusion_attention_list.append(fusion_attention[i])\n","                nb_test_examples += input_ids.size(0)\n","                nb_test_steps += 1\n","\n","            cmbert_output_tmp = np.array(logits_list)\n","            #np.save('cmbert_output_tmp.npy', cmbert_output_tmp)\n","\n","            cmbert_output = []\n","            for i in range(len(cmbert_output_tmp)):\n","                cmbert_output.extend(cmbert_output_tmp[i])\n","            np.save('cmbert_output.npy', cmbert_output)\n","\n","            predict_list = np.array(predict_list).reshape(-1)\n","            truth_list = np.array(truth_list)\n","\n","            np.save('cmbert_pred.npy', predict_list)\n","            np.save('cmbert_truth.npy', truth_list)\n","\n","            f_score = f1_score(truth_list, predict_list, average='weighted')\n","            acc = accuracy_score(predict_list, truth_list)\n","\n","            results = {'accuracy': acc, 'F1 score': f_score}\n","\n","        return results\n","\n","if __name__ == \"__main__\":\n","    results = main(1)\n","    print(results)\n"],"metadata":{"id":"oDI1mHhsaAVQ","executionInfo":{"status":"error","timestamp":1724296126515,"user_tz":-540,"elapsed":1756,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"colab":{"base_uri":"https://localhost:8080/","height":456},"outputId":"d8e6881e-a36d-409b-84e5-c5cc9ef7ce29"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n"]},{"output_type":"error","ename":"TypeError","evalue":"not a string","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-dcab3dbf2d7d>\u001b[0m in \u001b[0;36m<cell line: 259>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-dcab3dbf2d7d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKoBERTTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### KoBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mtrain_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2161\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2164\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2395\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m             raise OSError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kobert_tokenizer/kobert_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens, sp_model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msp_model_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msp_model_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlnet/tokenization_xlnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens, sp_model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         super().__init__(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    903\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_EncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_eos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_unk_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: not a string"]}]},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","\n","model = BertModel.from_pretrained('monologg/kobert')\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n"],"metadata":{"id":"INImTTOSlS0P","executionInfo":{"status":"ok","timestamp":1724294758633,"user_tz":-540,"elapsed":1886,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"63f97bde-861b-49e0-cd3a-bb33b955fd41"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-9rbrMFN9L75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if n_gpu > 1:\n","    model = torch.nn.DataParallel(model)\n","\n","param_optimizer = list(model.named_parameters())\n","\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","new_decay = ['BertFine']\n","\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and not any(np in n for np in new_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay )and any(np in n for np in new_decay)],'lr':0.01}\n","]\n","\n","optimizer = BertAdam(optimizer_grouped_parameters,\n","                     lr=args.learning_rate,\n","                     warmup=args.warmup_proportion,\n","                     t_total=num_train_optimization_steps)\n","\n","global_step = 0\n","nb_tr_steps = 0\n","tr_loss = 0\n","\n","audio_npy = np.load('/content/drive/MyDrive/송여경/audio-001.npy')\n","train_audio, test_audio = audio_npy[:8231], audio_npy[8231:]\n","\n","if args.do_train:\n","\n","    train_features = convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer)\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_examples))\n","    logger.info(\"  Batch size = %d\", args.train_batch_size)\n","    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n","    all_train_audio = torch.tensor(train_audio, dtype=torch.float32)\n","    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float32)\n","\n","    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_train_audio, all_label_ids)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","    for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n","        model.train()\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n","            batch = tuple(t.to(device) for t in batch)\n","            input_ids, input_mask, segment_ids, all_train_audio, label_ids = batch\n","            loss = model(input_ids, all_train_audio, segment_ids, input_mask, label_ids)\n","            if n_gpu > 1:\n","                loss = loss.mean()\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","\n","            tr_loss += loss.item()\n","            nb_tr_examples += input_ids.size(0)\n","            nb_tr_steps += 1\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                global_step += 1\n","\n","if args.do_test:\n","\n","    test_examples = processor.get_test_examples(args.data_dir)\n","    test_features = convert_examples_to_features(test_examples, label_list, args.max_seq_length, tokenizer)\n","    logger.info(\"\")\n","    logger.info(\"***** Running test *****\")\n","    logger.info(\"  Num examples = %d\", len(test_examples))\n","    logger.info(\"  Batch size = %d\", args.test_batch_size)\n","\n","    all_test_audio = torch.tensor(test_audio, dtype=torch.float32, requires_grad=True)\n","    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.float32)\n","\n","    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_test_audio)\n","\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.test_batch_size)\n","\n","    model.eval()\n","\n","    test_loss, test_accuracy = 0, 0\n","    nb_test_steps, nb_test_examples = 0, 0\n","    predict_list = []\n","    truth_list = []\n","    text_attention_list = []\n","    fusion_attention_list = []\n","\n","    logits_list = []\n","\n","    with torch.no_grad():\n","        for input_ids, input_mask, segment_ids, label_ids, all_test_audio in tqdm(test_dataloader, desc=\"Evaluating\"):\n","            input_ids = input_ids.to(device)\n","            input_mask = input_mask.to(device)\n","            segment_ids = segment_ids.to(device)\n","            label_ids = label_ids.to(device)\n","            all_test_audio = all_test_audio.to(device)\n","\n","            with torch.no_grad():\n","                tmp_test_loss = model(input_ids, all_test_audio, segment_ids, input_mask, label_ids)\n","                logits, text_attention, fusion_attention = model(input_ids, all_test_audio, segment_ids, input_mask)\n","\n","            logits_tmp = logits.detach().cpu().numpy()\n","\n","            logits_list.append(logits_tmp)\n","\n","            logits = torch.argmax(logits, dim=1)\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = label_ids.to('cpu').numpy()\n","            text_attention = text_attention.cpu().numpy()\n","            fusion_attention = fusion_attention.cpu().numpy()\n","            test_loss += tmp_test_loss.mean().item()\n","\n","            for i in range(len(logits)):\n","                predict_list.append(logits[i])\n","                truth_list.append(label_ids[i])\n","                text_attention_list.append(text_attention[i])\n","                fusion_attention_list.append(fusion_attention[i])\n","            nb_test_examples += input_ids.size(0)\n","            nb_test_steps += 1\n","\n","        cmbert_output_tmp = np.array(logits_list)\n","        #np.save('cmbert_output_tmp.npy', cmbert_output_tmp)\n","\n","        cmbert_output = []\n","        for i in range(len(cmbert_output_tmp)):\n","            cmbert_output.extend(cmbert_output_tmp[i])\n","        np.save('cmbert_output.npy', cmbert_output)\n","\n","        predict_list = np.array(predict_list).reshape(-1)\n","        truth_list = np.array(truth_list)\n","\n","        np.save('cmbert_pred.npy', predict_list)\n","        np.save('cmbert_truth.npy', truth_list)\n","\n","        f_score = f1_score(truth_list, predict_list, average='weighted')\n","        acc = accuracy_score(predict_list, truth_list)\n","\n","        results = {'accuracy': acc, 'F1 score': f_score}\n","\n","    return results\n","\n","if __name__ == \"__main__\":\n","    results = main(1)\n","    print(results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"YqkpuoabTwl1","executionInfo":{"status":"error","timestamp":1724294384423,"user_tz":-540,"elapsed":532,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"e1afde19-2e8d-4157-a789-771a4867b8e1"},"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'n_gpu' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-b3d446ca4b27>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparam_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'n_gpu' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"id":"DzO6IMZOlXwN","executionInfo":{"status":"error","timestamp":1723988948708,"user_tz":-540,"elapsed":357,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"b0f77c9b-24f5-456a-f535-d7ff4908cd8b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'args' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-36101a0205a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = BertForSequenceClassification.from_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'./content/kobert-base-v1/'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 로컬 경로로 수정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","def load_and_process_data(file_path, tokenizer, max_seq_length):\n","    data = pd.read_csv(file_path, delimiter='\\t')\n","    input_ids = []\n","    attention_masks = []\n","    labels = []\n","\n","    for _, row in data.iterrows():\n","        encoded_dict = tokenizer.encode_plus(\n","            row['text'],  # 문장\n","            add_special_tokens=True,  # [CLS]와 [SEP] 추가\n","            max_length=max_seq_length,  # 최대 길이\n","            pad_to_max_length=True,\n","            return_attention_mask=True,  # 어텐션 마스크 반환\n","            return_tensors='pt',  # PyTorch 텐서 반환\n","        )\n","\n","        input_ids.append(encoded_dict['input_ids'])\n","        attention_masks.append(encoded_dict['attention_mask'])\n","        labels.append(row['label'])\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","\n","    return TensorDataset(input_ids, attention_masks, labels)\n"],"metadata":{"id":"RADJZT8YrZud","executionInfo":{"status":"ok","timestamp":1724281494459,"user_tz":-540,"elapsed":790,"user":{"displayName":"송여경","userId":"16440594923547462791"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from tqdm import tqdm, trange\n","from transformers import AdamW, BertForSequenceClassification, BertTokenizer, BertConfig\n","from sklearn.metrics import f1_score, accuracy_score\n","import os\n","import random\n","\n","class PgProcessor:\n","    def get_train_examples(self, data_dir):\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"/content/drive/MyDrive/송여경/train.tsv\")), \"train\")\n","\n","    def get_test_examples(self, data_dir):\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"/content/drive/MyDrive/송여경/test.tsv\")), \"test\")\n","\n","    def get_labels(self):\n","        return [\"disgust\", \"sad\", \"anger\", \"fear\", \"neutral\"]  # 실제 레이블 목록으로 수정해야 합니다.\n","\n","    def _create_examples(self, lines, set_type):\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            guid = \"%s-%s\" % (set_type, i)\n","            text_a = line[0]\n","            label = line[1]\n","            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","        return examples\n","\n","    def _read_tsv(self, input_file, quotechar=None):\n","        \"\"\"탭으로 구분된 값(TSV) 파일을 읽습니다.\"\"\"\n","        data = []\n","        with open(input_file, \"r\", encoding='utf-8') as f:\n","            next(f)  # 첫 번째 줄(헤더)을 건너뜁니다.\n","            for line in f:\n","                # 라인 끝에 있는 여분의 공백이나 특수 문자 제거\n","                line = line.strip()\n","\n","                # 줄에서 탭으로 분리\n","                parts = line.split(\"\\t\")\n","\n","                # 각 부분에서 불필요한 공백과 쉼표 제거\n","                parts = [part.strip().rstrip(',') for part in parts if part.strip()]\n","\n","                # 라인이 정확히 두 부분(텍스트와 레이블)으로 나뉘는지 확인합니다.\n","                if len(parts) != 2:\n","                    print(f\"형식이 잘못되어 라인을 건너뜁니다: {line}\")\n","                    continue\n","\n","                # 두 번째 부분을 정수형 레이블로 변환을 시도합니다.\n","                try:\n","                    text = parts[0]\n","                    label = int(parts[1])\n","                    data.append([text, label])\n","                except ValueError:\n","                    print(f\"변환 오류로 인해 라인을 건너뜁니다: {line}\")\n","                    continue\n","\n","        return data\n","\n","\n","\n","class InputExample:\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label\n","\n","def load_and_process_data(file_path, tokenizer, max_seq_length, chunk_size=1000):\n","    processor = PgProcessor()\n","    examples = processor.get_train_examples(file_path) if 'train' in file_path else processor.get_test_examples(file_path)\n","\n","    for i in range(0, len(examples), chunk_size):\n","        chunk_examples = examples[i:i+chunk_size]\n","        features = convert_examples_to_features(chunk_examples, processor.get_labels(), max_seq_length, tokenizer)\n","\n","        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n","\n","        yield TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","\n","def main(i):\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument(\"--data_dir\", default='data/', type=str,\n","                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n","    parser.add_argument(\"--bert_model\", default='monologg/kobert', type=str,\n","                        help=\"Bert pre-trained model selected in the list.\")\n","    parser.add_argument(\"--task_name\", default='Multi', type=str,\n","                        help=\"The name of the task to train.\")\n","    parser.add_argument(\"--output_dir\", default='CM-BERT_output/', type=str,\n","                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n","    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n","                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n","    parser.add_argument(\"--max_seq_length\", default=100, type=int,\n","                        help=\"The maximum total input sequence length after WordPiece tokenization.\")\n","    parser.add_argument(\"--do_train\", action='store_true', default=True,\n","                        help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_test\", action='store_true', default=True,\n","                        help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_lower_case\", action='store_true',\n","                        help=\"Set this flag if you are using an uncased model.\")\n","    parser.add_argument(\"--train_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for training.\")\n","    parser.add_argument(\"--eval_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for eval.\")\n","    parser.add_argument(\"--test_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for test.\")\n","    parser.add_argument(\"--learning_rate\", default=2e-5, type=float,\n","                        help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--num_train_epochs\", default=3.0, type=float,\n","                        help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float,\n","                        help=\"Proportion of training to perform linear learning rate warmup for.\")\n","    parser.add_argument(\"--no_cuda\", action='store_true',\n","                        help=\"Whether not to use CUDA when available\")\n","    parser.add_argument('--seed', type=int, default=11111,\n","                        help=\"Random seed for initialization\")\n","    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n","                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n","\n","    args = parser.parse_args(args=[])\n","\n","    processors = {\n","        \"multi\": PgProcessor,\n","    }\n","    num_labels_task = {\n","        \"multi\": 7,\n","    }\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","    n_gpu = torch.cuda.device_count()\n","    print(\"device: {} n_gpu: {}\".format(device, n_gpu))\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(args.gradient_accumulation_steps))\n","\n","    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n","\n","    seed_num = np.random.randint(1, 10000)\n","    random.seed(seed_num)\n","    np.random.seed(seed_num)\n","    torch.manual_seed(seed_num)\n","    if n_gpu > 0:\n","        torch.cuda.manual_seed_all(seed_num)\n","\n","    if not args.do_train and not args.do_test:\n","        raise ValueError(\"At least one of `do_train` or `do_test` must be True.\")\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    task_name = args.task_name.lower()\n","\n","    if task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (task_name))\n","\n","    processor = processors[task_name]()\n","    num_labels = num_labels_task[task_name]\n","    label_list = processor.get_labels()\n","\n","    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n","\n","    model = BertForSequenceClassification.from_pretrained(\n","        args.bert_model,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","        num_labels=num_labels\n","    )\n","\n","    if n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    param_optimizer = list(model.named_parameters())\n","\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    new_decay = ['BertFine']\n","\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and not any(np in n for np in new_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","        {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and any(np in n for np in new_decay)],'lr':0.01}\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n","\n","    global_step = 0\n","    nb_tr_steps = 0\n","    tr_loss = 0\n","\n","    if args.do_train:\n","        for train_dataset in load_and_process_data(os.path.join(args.data_dir, \"/content/drive/MyDrive/train.tsv\"), tokenizer, args.max_seq_length):\n","            train_sampler = RandomSampler(train_dataset)\n","            train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","            num_train_optimization_steps = len(train_dataloader) // args.gradient_accumulation_steps * int(args.num_train_epochs)\n","\n","            print(\"***** Running training *****\")\n","            print(\"  Num examples = %d\" % len(train_dataset))\n","            print(\"  Batch size = %d\" % args.train_batch_size)\n","            print(\"  Num steps = %d\" % num_train_optimization_steps)\n","\n","            for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n","                model.train()\n","                tr_loss = 0\n","                nb_tr_examples, nb_tr_steps = 0, 0\n","                for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n","                    batch = tuple(t.to(device) for t in batch)\n","                    input_ids, input_mask, labels = batch\n","                    outputs = model(input_ids, attention_mask=input_mask, labels=labels)\n","                    loss = outputs[0]\n","\n","                    if n_gpu > 1:\n","                        loss = loss.mean()\n","                    if args.gradient_accumulation_steps > 1:\n","                        loss = loss / args.gradient_accumulation_steps\n","\n","                    loss.backward()\n","\n","                    tr_loss += loss.item()\n","                    nb_tr_examples += input_ids.size(0)\n","                    nb_tr_steps += 1\n","                    if (step + 1) % args.gradient_accumulation_steps == 0:\n","                        optimizer.step()\n","                        optimizer.zero_grad()\n","                        global_step += 1\n","\n","    if args.do_test:\n","        for test_dataset in load_and_process_data(os.path.join(args.data_dir, \"/content/drive/MyDrive/test.tsv\"), tokenizer, args.max_seq_length):\n","            test_sampler = SequentialSampler(test_dataset)\n","            test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.test_batch_size)\n","\n","            print(\"***** Running test *****\")\n","            print(\"  Num examples = %d\" % len(test_dataset))\n","            print(\"  Batch size = %d\" % args.test_batch_size)\n","\n","            model.eval()\n","\n","            predict_list = []\n","            truth_list = []\n","\n","            with torch.no_grad():\n","                for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n","                    batch = tuple(t.to(device) for t in batch)\n","                    input_ids, input_mask, labels = batch\n","\n","                    outputs = model(input_ids, attention_mask=input_mask)\n","                    logits = outputs[0]\n","\n","                    logits = torch.argmax(logits, dim=1)\n","                    logits = logits.detach().cpu().numpy()\n","                    labels = labels.to('cpu').numpy()\n","\n","                    predict_list.extend(logits)\n","                    truth_list.extend(labels)\n","\n","            predict_list = np.array(predict_list)\n","            truth_list = np.array(truth_list)\n","\n","            np.save('cmbert_pred.npy', predict_list)\n","            np.save('cmbert_truth.npy', truth_list)\n","\n","            f_score = f1_score(truth_list, predict_list, average='weighted')\n","            acc = accuracy_score(predict_list, truth_list)\n","\n","            results = {'accuracy': acc, 'F1 score': f_score}\n","\n","            print(results)\n","\n","if __name__ == \"__main__\":\n","    main(1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UMig8D21bGDx","executionInfo":{"status":"error","timestamp":1724283221156,"user_tz":-540,"elapsed":2393,"user":{"displayName":"송여경","userId":"16440594923547462791"}},"outputId":"b08e8d25-a696-4502-dd58-bdd07ef164b3"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["device: cuda n_gpu: 0\n"]},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려 있길래 궁금해서 구경 다녀 왔어,\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"네, 그래야겠어요.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"네, 부탁드려요 기분이 좀 그러네요.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"나도 궁금해서 쫓아 갔었는데 결국엔 못 봤어,\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 여행 계획도 세우고 싶다,\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"사람들이 많이 놀라긴 했는데 그래도 다 강아지를 싫어하지 않더라고.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"다행히 멀리 못 가서 다시 목줄를 묵고 다시 집으로 돌아왔어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"많이 무서워 어떻게 할지 모르겠어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"샀을 때보다 30% 정도 떨어졌어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"화장실은 청소를 해도 곰팡이가 없어 지지를 않아.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"오늘 바로 구매하려고 좋은 생각 고마워.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"밥도 잘 나가서 못 먹겠고 운동도 꾸준히 했었는데 이제 잘 안 나가게 되네.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"이전에 음식물 쓰레기를 버렸었는데 그게 썩은 거 같아 냄새가 많이 나더라.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"그러게 주말인데도 수십 명이 몰려 있으니까.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려 있길래 궁금해서 구경 다녀 왔어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려있길래 궁금해서 구경 다녀 왔어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"코로나 때문에 일 년 동안이나 여행을 못 갔는데 갑자기 또 많이 확진자가 증가 해서 너무 우울해.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"그럼 다음 주 쯤에 제주도 여행이나 한번가 볼까?\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"일단 전화를 받아 보고 다시 할지 말지는 생각해 봐야 되는데 그냥 한동안은 끝났어\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"어떡해 지금 엘리베이터에 갇혔어 너무 무서워.\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"오늘 저녁에 당장 약속 잡아야지 안 되겠어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"여행은 좀 힘들고 좀 많이 쉬어야겠어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"몸이 무기력해져서 어떻게 해야 하지?\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"응 좋은 운동 좀 찾아 봐 줘.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"일단 119에 신고는 했는데 무서워도 참아야지 어떻게 해.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"조금 놀랐을 뿐이고 몸은 괜찮아 너무 힘들어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"그래 다음에 한번 보면은 내가 향수 뿌리고 나가 볼게.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"답답하지 코로나 때문에 일도 불안정하고 마음대로 되는게 없네.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"원래 돌아다니는 것도 좋아하는데 코로나 때문에 돌아다니지도 못 하니까 삶의 의지가 없다.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"코로나 때문에 밖에 나가지도 못하고 재택근무 하고 있고 나가지 못한 거 그런 거 같아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"헐 나 이벤트 당첨됐어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"그러니까 퇴근하고 또 혼자 엘리베이터를 탔는데 갑자기 정전이 나 가지고 엘리베이터 멈춰 버렸어 긴급전화는 아무도 안 받아 어떻게 해?\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"신나는 음악 보다는 맛있는 음식을 먹고 싶어 못 먹은 음식이 너무 많아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"어, 퇴근에 쉬지도 못하고 막 야근하면서 열심히 했는데 잘 마무리 된 거 같아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려 있길래 구경하다가 왔어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아니 그건 아니지 내가 겪어도 기분이 나쁠 만 해.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아니 기분이 나아지지는 않아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"나 너무 우울해.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"그래도 짭새가 싫은 걸 어떡해.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"코로나 때문에 일자리도 잃고 집 밖으로 나가지도 못 해서 너무 우울한 거 같아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"응, 오늘 날씨는 좋은데 많이 추워.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"경품 추첨 했는데 청소기 받았어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"과제가 너무 많아 쉬고 싶어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"지금 마침 청소기가 필요했는데 청소기 이벤트 당첨됐어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"응 나는 마라톤 동호회에서 같이하고 있어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"나 너무 우울해.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"응 마음에 들어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"요즘 그냥 평소랑 똑같이 하고 있지 뭐.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"그래 오늘은 스트레칭 해야겠다.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 그래도 이제 두 개 남아서 다행이야.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 결승선에서 기분이 너무 좋아서 계속 마라톤 하게 돼.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"아 그러면 안 되는데.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"그냥 경찰들 많이 모여있길래 무슨 일 있나 싶어 가지고.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"몇 명 뽑는지는 잘 모르겠다.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"저번에 음식물 쓰레기 모르고 버린 것 때문에 그런 거 같아.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"저번에 사과 먹고 사과 껍데기 쓰레기통에 버렸더니 그거 썩는 냄새 같아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아니 됐어 다시 전화하기도 싫다.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"사진 안 찍었는데 어떡하지 환불 안해주려나.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"거기 다시는 안 시킬 거야.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"짜장면에서 벌레 나왔다.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아니야 지금 그냥 빨리 뺄까 생각이야.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아 나 어제는 또 미세먼지 너무 심해 가지고.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 그냥 쓰레기통 버리고 종량제 봉투 새로 사려고.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"환기 엄청 많이 시키지 근데 창문이 너무 작아 가지고 소용이 없는 거 같아.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"일단 환기 좀 시키고 나서.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"언제 한번 상사한테 말해 볼까 고민이야.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 알고 있지 그런 말이 잘 안 쓸게.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"어떻게 된게 화장실은 청소를 해도 곰팡이가 없어지질않아.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"아 맞아 미안해 말이 잘못 나왔어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려 있길래 궁금해서 구경 다녀 왔어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"몸도 무기력한 거 같아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"좋은 생각인 거 같다 몸을 움직이는게 나을 거 같아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"당연하지 사진 다 찍어 놨어.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"나 혼자야 너무 무서워.\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"모르겠어 나도 이런 적 처음이라서 정전 됐나 봐.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"나 엘리베이터에 갇혔어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"나 드디어 프로젝트 끝났어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"응 답답하고 우울해.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"기분 완전 별로야 다시는 짜장면 먹고 싶지 않아.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"혼자 있어 나 너무 무서워.\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"응 몸은 괜찮아 근데 지금 너무 무서워.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려 있길래 구경하다가 왔어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"은행에 강도가 들었었나봐.\t\"\t2\n","형식이 잘못되어 라인을 건너뜁니다: \"응 엄청 큰 애 같더라고.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"사진은 찍어 놨지.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 그래서 지금 너무 개운해.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"그것도 좋은 거 같아 근데 지금은 조금 싫네.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"좋은 생각인데 나는 말을 잘 못 해.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"그것도 나쁘지 않을 거 같아.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"좋을 거 같아 근데 어떤 노래를 듣지?\t\"\t2\n","형식이 잘못되어 라인을 건너뜁니다: \"근데 상사니깐 말도 못 하고 그냥 다녀야지 뭐 짜증 난다.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"아 짜증나 아니 그 새끼는 할 일 없으면 집에나 갈 것이지 왜 나한테만 난리야.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"그 집 이제 더 안 시켜 먹을거야 아 짜증 나.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"그래야겠다 아주 중국집에 전화해서 갖고 가라 할 거야 아 나 진짜.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"그럼 그러면 기분이 좀 나아지려나.\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"나 축하해 줘 나 기분 좋은 일 있어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"나 마라톤 기록 갱신했다.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"진짜 너무 힘들었는데 딱 들어가는 순간 진짜 너무 행복한거 있지!\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"너무 무기력해 뭘 해야 될지 모르겠어 나 있지.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아 진짜 코로나 언제 끝나는 거야 나 진짜 나 가고 싶어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아니 이번에는 혼자 가서 했어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"그럼 그럼 얼마나 홀가분한지 몰라.\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"지인 추천으로 시작을 하게 됐어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"나 요즘 주식을 시작했어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"오 맞아 맞아 진짜 힘들게 연습한 보람이 있었어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"나 드디어 프로젝트 끝났어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"환기도 자주 확인하는데 창문이 너무 작은가 봐.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"다친 데는 없는데 너무 깜깜해서 무서워.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"혼자 있는데 나 어떡해.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"엘리베이터 타다가 갑자기 멈췄어 어떡하지 너무 무서워.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"나 엘리베이터에 갇혔어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 이건 진짜 다시 배달해 줘야해.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"몸도 무기력하고 아무것도 하기 싫어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"그렇구나 우울한데 어떡하면 좋을까?\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"코로나로 사람들 못 만나서 우울해\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"화장실은 청소를 해도 곰팡이가 없어지질 않아.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"그냥 짭새들이 엄청 몰려 있는 거야 그래서 무슨 일 있나 싶었지.\t\"\t2\n","형식이 잘못되어 라인을 건너뜁니다: \"맞아 미세먼지도 걱정이 되고 곰팡이가 계속 생기니까 너무 그렇다.\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려 있길래 궁금해서 구경 다녀 왔어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"헐 나 이벤트 당첨됐어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"나 오늘 짭새가 몰려 있길래 궁금해서 구경 다녀왔어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"아 진짜 짜증나 아니 그 사람은 할 일 없으면 집에 나갈 것이지 왜 자꾸 나한테 일 시키고 난리야.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"어디로 가는 것이 좋을까?\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"다행히 아무 일도 없었어.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"다행히 아무도 안 다쳐서 다행이야.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"갑자기 목줄이 끊어졌어.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"자세히는 모르겠는데 진짜 조금 뽑는 거였을걸?\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"향수 진짜 몇 명 안 주는 건데 그거 당첨됐다니까.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"모르겠어 나 이제 어떻게 해야 돼?\t\"\t6\n","형식이 잘못되어 라인을 건너뜁니다: \"다른 건 없어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"오랜만에 친구도 만나야겠다.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"이미 노래 틀고 있지.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"이제 완전 끝난 거야.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"나도 프로젝트 끝났어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"다음에 한번 보여 줄게.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"응 선물 너무 마음에 들어.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"그냥 이벤트 인터넷으로 신청했는데 당첨된 거야.\t\"\t4\n","형식이 잘못되어 라인을 건너뜁니다: \"나 이벤트 당첨됐어.\t\"\t2\n","형식이 잘못되어 라인을 건너뜁니다: \"다행히 다친 곳은 없어.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"너무 무서워.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"지금 나 혼자 있어 갖고 너무 무서워.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"타다가 갑자기 정전돼 가지고 막 멈췄어 그래서 지금 갇힌 상태야.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"밖에 나가고 싶은데 나가면 또 걸릴까 봐 그렇지.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"계속 집에만 있다 보니까 많이 무기력해진 거 같아.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"어 그거 정말 좋은 생각이다.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"고마워 정말 고마워 정말 난감 했거든.\t\"\t1\n","형식이 잘못되어 라인을 건너뜁니다: \"나 정말 속상해.\t\"\t3\n","형식이 잘못되어 라인을 건너뜁니다: \"아 정말로? 그래야 되겠다.\t\"\t0\n","형식이 잘못되어 라인을 건너뜁니다: \"환불받고 다른 거 다 시키면 될 거 같아.\t\"\t0\n","***** Running training *****\n","  Num examples = 1000\n","  Batch size = 24\n","  Num steps = 126\n"]},{"output_type":"stream","name":"stderr","text":["Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n","Iteration:   0%|          | 0/42 [00:00<?, ?it/s]\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 3)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-6a2e8d827c01>\u001b[0m in \u001b[0;36m<cell line: 274>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-6a2e8d827c01>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"]}]},{"cell_type":"code","source":["# 현재 작업 디렉토리 확인\n","import os\n","print(os.getcwd())\n","\n","# 해당 디렉토리에 model.py가 있는지 확인\n","print(os.listdir())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NnrkJOWseD5v","executionInfo":{"status":"ok","timestamp":1723984325195,"user_tz":-540,"elapsed":326,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"0e710617-c9a9-4f54-f865-48b6d6ae1faa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","['.config', '__pycache__', 'model.py', 'utils.py', 'main.py', 'drive', 'CM-BERT_output', 'sample_data']\n"]}]}]}