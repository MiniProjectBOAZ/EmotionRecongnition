{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1m4s2G57XtLpny7qyVf0vFX7iHagISCax","authorship_tag":"ABX9TyMpmvSSCcu6op37XXIOMqhk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XeLVPguP1b_","executionInfo":{"status":"ok","timestamp":1723991400417,"user_tz":-540,"elapsed":7045,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"00dc59a0-e8f2-4d3b-b84d-8857ad42b5a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.10/dist-packages (0.6.2)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.3.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.26.4)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.35.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2024.5.15)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2024.6.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch_pretrained_bert) (12.6.20)\n","Requirement already satisfied: botocore<1.36.0,>=1.35.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.35.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2024.7.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.0->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.0->boto3->pytorch_pretrained_bert) (1.16.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"]}],"source":["!pip install pytorch_pretrained_bert\n","!pip install transformers\n","\n","from __future__ import absolute_import, division, print_function\n","import argparse\n","import os\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","from tqdm import tqdm, trange\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE,cached_path\n","from model import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n","from transformers import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert.optimization import BertAdam\n","from utils import *"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Google Drive에서 파일을 복사\n","!cp /content/drive/MyDrive/your_folder/model.py .\n","!cp /content/drive/MyDrive/your_folder/utils.py ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xzh6wEjRvGR","executionInfo":{"status":"ok","timestamp":1723982158058,"user_tz":-540,"elapsed":2520,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"202ae83e-bb0c-4731-e1b5-63e74f9bc49f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","cp: cannot stat '/content/drive/MyDrive/your_folder/model.py': No such file or directory\n","cp: cannot stat '/content/drive/MyDrive/your_folder/utils.py': No such file or directory\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/')\n","\n","!cp /content/drive/MyDrive/main.py .\n","!cp /content/drive/MyDrive/utils.py .\n","!cp /content/drive/MyDrive/model.py .\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CZ87vMoRDw7","executionInfo":{"status":"ok","timestamp":1723991391466,"user_tz":-540,"elapsed":870,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"f0c45daf-48f0-412c-bedb-869fce143601"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CM-BERT_output\tdrive  main.py\tmodel.py  __pycache__  sample_data  utils.py\n"]}]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n","!pip show kobert_tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBZqG57dP9dw","executionInfo":{"status":"ok","timestamp":1723991427778,"user_tz":-540,"elapsed":13673,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"5243e91a-639a-469b-e9f2-131731e60058"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-tw7jjsol/kobert-tokenizer_8b667324466543b3b18c3301ffff997e\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-tw7jjsol/kobert-tokenizer_8b667324466543b3b18c3301ffff997e\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Name: kobert-tokenizer\n","Version: 0.1\n","Summary: Korean BERT pre-trained cased (KoBERT) for HuggingFace \n","Home-page: https://github.com/SKTBrain/KoBERT\n","Author: SeungHwan Jung\n","Author-email: digit82@gmail.com\n","License: Apache-2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: \n","Required-by: \n"]}]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, accuracy_score\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"],"metadata":{"id":"v2BPDd8uQA8m","executionInfo":{"status":"ok","timestamp":1723991449243,"user_tz":-540,"elapsed":342,"user":{"displayName":"Hearo","userId":"14438065119341798053"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"iK0WUngwQAzE","executionInfo":{"status":"ok","timestamp":1723983213460,"user_tz":-540,"elapsed":353,"user":{"displayName":"Hearo","userId":"14438065119341798053"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def main(i):\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument(\"--data_dir\", default='data/', type=str,\n","                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n","    parser.add_argument(\"--bert_model\", default='kobert-base-v1/', type=str,\n","                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n","                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n","                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n","    parser.add_argument(\"--task_name\", default='Multi', type=str,\n","                        help=\"The name of the task to train.\")\n","    parser.add_argument(\"--output_dir\", default='CM-BERT_output/', type=str,\n","                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n","\n","    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n","                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n","    parser.add_argument(\"--max_seq_length\", default=100, type=int,\n","                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n","                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n","                             \"than this will be padded.\")\n","    parser.add_argument(\"--do_train\", action='store_true',\n","                        help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_test\", action='store_true',\n","                        help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_lower_case\", action='store_true',\n","                        help=\"Set this flag if you are using an uncased model.\")\n","    parser.add_argument(\"--train_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for training.\")\n","    parser.add_argument(\"--eval_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for eval.\")\n","    parser.add_argument(\"--test_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for test.\")\n","    parser.add_argument(\"--learning_rate\", default=2e-5, type=float,\n","                        help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--num_train_epochs\", default=3.0, type=float,\n","                        help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float,\n","                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n","                             \"E.g., 0.1 = 10%% of training.\")\n","    parser.add_argument(\"--no_cuda\", action='store_true',\n","                        help=\"Whether not to use CUDA when available\")\n","    parser.add_argument('--seed', type=int, default=11111,\n","                        help=\"Random seed for initialization\")\n","    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n","                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n","\n","    args = parser.parse_args()\n","\n","    processors = {\n","        \"multi\": PgProcessor,\n","    }\n","    num_labels_task = {\n","        \"multi\": 7,\n","    }\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","    n_gpu = torch.cuda.device_count()  # GPU의 수를 가져옵니다.\n","    logger.info(\"device: {} n_gpu: {}\".format(device, n_gpu))\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(args.gradient_accumulation_steps))\n","\n","    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n","\n","    seed_num = np.random.randint(1, 10000)\n","    random.seed(seed_num)\n","    np.random.seed(seed_num)\n","    torch.manual_seed(seed_num)\n","    if n_gpu > 0:\n","        torch.cuda.manual_seed_all(seed_num)\n","\n","    if not args.do_train and not args.do_test:\n","        raise ValueError(\"At least one of `do_train` or `do_test` must be True.\")\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    task_name = args.task_name.lower()\n","\n","    if task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (task_name))\n","\n","    processor = processors[task_name]()\n","    num_labels = num_labels_task[task_name]\n","    label_list = processor.get_labels()\n","\n","    tokenizer = KoBERTTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) ### KoBERT\n","\n","    train_examples = None\n","    num_train_optimization_steps = None\n","    if args.do_train:\n","        train_examples = processor.get_train_examples(args.data_dir)\n","        num_train_optimization_steps = int(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n","\n","\n","    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(\"-1\"))\n","\n","    model = BertForSequenceClassification.from_pretrained(args.bert_model, cache_dir=cache_dir, num_labels = num_labels)\n","\n","    for name, param in model.named_parameters():\n","        param.requires_grad = False\n","        if \"encoder.layer.0\" in name or \"encoder.layer.1\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.2\" in name or \"encoder.layer.3\" in name :\n","            param.requires_grad = True\n","        if \"encoder.layer.4\" in name or  \"encoder.layer.5\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.6\" in name or \"encoder.layer.7\" in name:\n","            param.requires_grad = True\n","        if \"encoder.layer.8\" in name or \"encoder.layer.9\" in name :\n","            param.requires_grad = True\n","        if \"encoder.layer.10\" in name or  \"encoder.layer.11\" in name:\n","            param.requires_grad = True\n","        if \"BertFinetun\" in name or \"pooler\" in name:\n","            param.requires_grad = True\n","\n","    model.to(device)\n"],"metadata":{"id":"oDI1mHhsaAVQ","executionInfo":{"status":"ok","timestamp":1723991452077,"user_tz":-540,"elapsed":358,"user":{"displayName":"Hearo","userId":"14438065119341798053"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","\n","model = BertModel.from_pretrained('monologg/kobert')\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n"],"metadata":{"id":"INImTTOSlS0P","executionInfo":{"status":"ok","timestamp":1723990334405,"user_tz":-540,"elapsed":3113,"user":{"displayName":"Hearo","userId":"14438065119341798053"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"id":"DzO6IMZOlXwN","executionInfo":{"status":"error","timestamp":1723988948708,"user_tz":-540,"elapsed":357,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"b0f77c9b-24f5-456a-f535-d7ff4908cd8b"},"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'args' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-36101a0205a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = BertForSequenceClassification.from_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'./content/kobert-base-v1/'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 로컬 경로로 수정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","def load_and_process_data(file_path, tokenizer, max_seq_length):\n","    data = pd.read_csv(file_path, delimiter='\\t')\n","    input_ids = []\n","    attention_masks = []\n","    labels = []\n","\n","    for _, row in data.iterrows():\n","        encoded_dict = tokenizer.encode_plus(\n","            row['text'],  # 문장\n","            add_special_tokens=True,  # [CLS]와 [SEP] 추가\n","            max_length=max_seq_length,  # 최대 길이\n","            pad_to_max_length=True,\n","            return_attention_mask=True,  # 어텐션 마스크 반환\n","            return_tensors='pt',  # PyTorch 텐서 반환\n","        )\n","\n","        input_ids.append(encoded_dict['input_ids'])\n","        attention_masks.append(encoded_dict['attention_mask'])\n","        labels.append(row['label'])\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","\n","    return TensorDataset(input_ids, attention_masks, labels)\n"],"metadata":{"id":"RADJZT8YrZud","executionInfo":{"status":"ok","timestamp":1723991458508,"user_tz":-540,"elapsed":1564,"user":{"displayName":"Hearo","userId":"14438065119341798053"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from tqdm import tqdm, trange\n","from transformers import AdamW, BertForSequenceClassification, BertTokenizer, BertConfig\n","from sklearn.metrics import f1_score, accuracy_score\n","import os\n","import random\n","\n","class PgProcessor:\n","    def get_train_examples(self, data_dir):\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n","\n","    def get_test_examples(self, data_dir):\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n","\n","    def get_labels(self):\n","        return [\"disgust\", \"sad\", \"anger\", \"fear\", \"neutral\"]  # 실제 레이블 목록으로 수정해야 합니다.\n","\n","    def _create_examples(self, lines, set_type):\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            guid = \"%s-%s\" % (set_type, i)\n","            text_a = line[0]\n","            label = line[1]\n","            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","        return examples\n","\n","    def _read_tsv(self, input_file, quotechar=None):\n","        with open(input_file, \"r\") as f:\n","            return [line.strip().split(\"\\t\") for line in f]\n","\n","class InputExample:\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label\n","\n","def load_and_process_data(file_path, tokenizer, max_seq_length, chunk_size=1000):\n","    processor = PgProcessor()\n","    examples = processor.get_train_examples(file_path) if 'train' in file_path else processor.get_test_examples(file_path)\n","\n","    for i in range(0, len(examples), chunk_size):\n","        chunk_examples = examples[i:i+chunk_size]\n","        features = convert_examples_to_features(chunk_examples, processor.get_labels(), max_seq_length, tokenizer)\n","\n","        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n","\n","        yield TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","\n","def main(i):\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument(\"--data_dir\", default='data/', type=str,\n","                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n","    parser.add_argument(\"--bert_model\", default='monologg/kobert', type=str,\n","                        help=\"Bert pre-trained model selected in the list.\")\n","    parser.add_argument(\"--task_name\", default='Multi', type=str,\n","                        help=\"The name of the task to train.\")\n","    parser.add_argument(\"--output_dir\", default='CM-BERT_output/', type=str,\n","                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n","    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n","                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n","    parser.add_argument(\"--max_seq_length\", default=100, type=int,\n","                        help=\"The maximum total input sequence length after WordPiece tokenization.\")\n","    parser.add_argument(\"--do_train\", action='store_true', default=True,\n","                        help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_test\", action='store_true', default=True,\n","                        help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_lower_case\", action='store_true',\n","                        help=\"Set this flag if you are using an uncased model.\")\n","    parser.add_argument(\"--train_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for training.\")\n","    parser.add_argument(\"--eval_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for eval.\")\n","    parser.add_argument(\"--test_batch_size\", default=24, type=int,\n","                        help=\"Total batch size for test.\")\n","    parser.add_argument(\"--learning_rate\", default=2e-5, type=float,\n","                        help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--num_train_epochs\", default=3.0, type=float,\n","                        help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float,\n","                        help=\"Proportion of training to perform linear learning rate warmup for.\")\n","    parser.add_argument(\"--no_cuda\", action='store_true',\n","                        help=\"Whether not to use CUDA when available\")\n","    parser.add_argument('--seed', type=int, default=11111,\n","                        help=\"Random seed for initialization\")\n","    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n","                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n","\n","    args = parser.parse_args(args=[])\n","\n","    processors = {\n","        \"multi\": PgProcessor,\n","    }\n","    num_labels_task = {\n","        \"multi\": 7,\n","    }\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","    n_gpu = torch.cuda.device_count()\n","    print(\"device: {} n_gpu: {}\".format(device, n_gpu))\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(args.gradient_accumulation_steps))\n","\n","    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n","\n","    seed_num = np.random.randint(1, 10000)\n","    random.seed(seed_num)\n","    np.random.seed(seed_num)\n","    torch.manual_seed(seed_num)\n","    if n_gpu > 0:\n","        torch.cuda.manual_seed_all(seed_num)\n","\n","    if not args.do_train and not args.do_test:\n","        raise ValueError(\"At least one of `do_train` or `do_test` must be True.\")\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    task_name = args.task_name.lower()\n","\n","    if task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (task_name))\n","\n","    processor = processors[task_name]()\n","    num_labels = num_labels_task[task_name]\n","    label_list = processor.get_labels()\n","\n","    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n","\n","    model = BertForSequenceClassification.from_pretrained(\n","        args.bert_model,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","        num_labels=num_labels\n","    )\n","\n","    if n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    param_optimizer = list(model.named_parameters())\n","\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    new_decay = ['BertFine']\n","\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and not any(np in n for np in new_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","        {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and any(np in n for np in new_decay)],'lr':0.01}\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n","\n","    global_step = 0\n","    nb_tr_steps = 0\n","    tr_loss = 0\n","\n","    if args.do_train:\n","        for train_dataset in load_and_process_data(os.path.join(args.data_dir, \"/content/drive/MyDrive/train.tsv\"), tokenizer, args.max_seq_length):\n","            train_sampler = RandomSampler(train_dataset)\n","            train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","            num_train_optimization_steps = len(train_dataloader) // args.gradient_accumulation_steps * int(args.num_train_epochs)\n","\n","            print(\"***** Running training *****\")\n","            print(\"  Num examples = %d\" % len(train_dataset))\n","            print(\"  Batch size = %d\" % args.train_batch_size)\n","            print(\"  Num steps = %d\" % num_train_optimization_steps)\n","\n","            for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n","                model.train()\n","                tr_loss = 0\n","                nb_tr_examples, nb_tr_steps = 0, 0\n","                for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n","                    batch = tuple(t.to(device) for t in batch)\n","                    input_ids, input_mask, labels = batch\n","                    outputs = model(input_ids, attention_mask=input_mask, labels=labels)\n","                    loss = outputs[0]\n","\n","                    if n_gpu > 1:\n","                        loss = loss.mean()\n","                    if args.gradient_accumulation_steps > 1:\n","                        loss = loss / args.gradient_accumulation_steps\n","\n","                    loss.backward()\n","\n","                    tr_loss += loss.item()\n","                    nb_tr_examples += input_ids.size(0)\n","                    nb_tr_steps += 1\n","                    if (step + 1) % args.gradient_accumulation_steps == 0:\n","                        optimizer.step()\n","                        optimizer.zero_grad()\n","                        global_step += 1\n","\n","    if args.do_test:\n","        for test_dataset in load_and_process_data(os.path.join(args.data_dir, \"/content/drive/MyDrive/test.tsv\"), tokenizer, args.max_seq_length):\n","            test_sampler = SequentialSampler(test_dataset)\n","            test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.test_batch_size)\n","\n","            print(\"***** Running test *****\")\n","            print(\"  Num examples = %d\" % len(test_dataset))\n","            print(\"  Batch size = %d\" % args.test_batch_size)\n","\n","            model.eval()\n","\n","            predict_list = []\n","            truth_list = []\n","\n","            with torch.no_grad():\n","                for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n","                    batch = tuple(t.to(device) for t in batch)\n","                    input_ids, input_mask, labels = batch\n","\n","                    outputs = model(input_ids, attention_mask=input_mask)\n","                    logits = outputs[0]\n","\n","                    logits = torch.argmax(logits, dim=1)\n","                    logits = logits.detach().cpu().numpy()\n","                    labels = labels.to('cpu').numpy()\n","\n","                    predict_list.extend(logits)\n","                    truth_list.extend(labels)\n","\n","            predict_list = np.array(predict_list)\n","            truth_list = np.array(truth_list)\n","\n","            np.save('cmbert_pred.npy', predict_list)\n","            np.save('cmbert_truth.npy', truth_list)\n","\n","            f_score = f1_score(truth_list, predict_list, average='weighted')\n","            acc = accuracy_score(predict_list, truth_list)\n","\n","            results = {'accuracy': acc, 'F1 score': f_score}\n","\n","            print(results)\n","\n","if __name__ == \"__main__\":\n","    main(1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"UMig8D21bGDx","executionInfo":{"status":"error","timestamp":1723991464389,"user_tz":-540,"elapsed":2667,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"c7fd7939-f7fb-415f-9eaa-3e0eaed559f6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["device: cuda n_gpu: 1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"error","ename":"NotADirectoryError","evalue":"[Errno 20] Not a directory: '/content/drive/MyDrive/train.tsv/train.tsv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-02562fc04971>\u001b[0m in \u001b[0;36m<cell line: 245>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-02562fc04971>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/train.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-02562fc04971>\u001b[0m in \u001b[0;36mload_and_process_data\u001b[0;34m(file_path, tokenizer, max_seq_length, chunk_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPgProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-02562fc04971>\u001b[0m in \u001b[0;36mget_train_examples\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         return self._create_examples(\n\u001b[0;32m---> 14\u001b[0;31m             self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_test_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-02562fc04971>\u001b[0m in \u001b[0;36m_read_tsv\u001b[0;34m(self, input_file, quotechar)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_tsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/MyDrive/train.tsv/train.tsv'"]}]},{"cell_type":"code","source":["# 현재 작업 디렉토리 확인\n","import os\n","print(os.getcwd())\n","\n","# 해당 디렉토리에 model.py가 있는지 확인\n","print(os.listdir())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NnrkJOWseD5v","executionInfo":{"status":"ok","timestamp":1723984325195,"user_tz":-540,"elapsed":326,"user":{"displayName":"Hearo","userId":"14438065119341798053"}},"outputId":"0e710617-c9a9-4f54-f865-48b6d6ae1faa"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","['.config', '__pycache__', 'model.py', 'utils.py', 'main.py', 'drive', 'CM-BERT_output', 'sample_data']\n"]}]}]}