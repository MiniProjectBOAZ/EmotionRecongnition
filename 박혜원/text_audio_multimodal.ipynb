{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGHnxdgIk57O",
        "outputId": "0025a909-9719-4b73-9652-64c0f864247d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO7bxy-Ck-mG",
        "outputId": "a170214d-374b-4923-ae61-5d3de241e82d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.26.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.35.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.66.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2024.5.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch-pretrained-bert) (12.6.20)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.35.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.0->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.0->boto3->pytorch-pretrained-bert) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 같은 전처리 과정을 담고 있는 코드\n",
        "# 텍스트 데이터 path 수정해야 함.\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from random import shuffle\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "class PgProcessor(DataProcessor):\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"/content/gdrive/MyDrive/BOAZ/mini_pj/train.tsv\")), \"train\") # 올바른 파일 경로 입력\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"/test.tsv\")), \"test\") # 올바른 파일 경로 입력\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[0]\n",
        "            label = line[1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        # label_id = label_map[example.label]\n",
        "        label_id = example.label\n",
        "        label_id = float(label_id)\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %s)\" % (example.label, label_id))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop(0) #For dialogue context\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "metadata": {
        "id": "BNFnVIlEMbG4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install transformers==4.8.2\n",
        "!pip install torch\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5tIGJYyLaff",
        "outputId": "ba80c1dd-ee3a-4996-c7b9-7b92173fde57"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.32.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.7.4)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.26.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Using cached sentencepiece-0.1.91.tar.gz (500 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting transformers==4.8.2\n",
            "  Using cached transformers-4.8.2-py3-none-any.whl.metadata (48 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (3.15.4)\n",
            "Collecting huggingface-hub==0.0.12 (from transformers==4.8.2)\n",
            "  Using cached huggingface_hub-0.0.12-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.8.2)\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.8.2)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (2024.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.8.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.8.2) (1.4.2)\n",
            "Using cached transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-a2w_jaeq\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-a2w_jaeq\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18 (from kobert==0.2.3)\n",
            "  Using cached boto3-1.15.18-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0 (from kobert==0.2.3)\n",
            "  Using cached mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "INFO: pip is looking at multiple versions of kobert to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime<=1.8.0,==1.8.0 (from kobert) (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.17.0, 1.17.1, 1.17.3, 1.18.0, 1.18.1, 1.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime<=1.8.0,==1.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-klns0cpm/kobert-tokenizer_757cee8c580447c598ecdb86a77afc6c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-klns0cpm/kobert-tokenizer_757cee8c580447c598ecdb86a77afc6c\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import argparse\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "import os\n",
        "import random\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "J1ncsSlOLFlB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args():\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.data_dir = '/content/gdrive/MyDrive/BOAZ/mini_pj/'\n",
        "            self.bert_model = 'skt/kobert-base-v1'\n",
        "            self.task_name = 'Multi'\n",
        "            self.output_dir = '/content/gdrive/MyDrive/BOAZ/mini_pj/CM-BERT_output/'\n",
        "            self.cache_dir = \"\"\n",
        "            self.max_seq_length = 100\n",
        "            self.do_train = True\n",
        "            self.do_test = True\n",
        "            self.do_lower_case = True\n",
        "            self.train_batch_size = 24\n",
        "            self.eval_batch_size = 24\n",
        "            self.test_batch_size = 24\n",
        "            self.learning_rate = 2e-5\n",
        "            self.num_train_epochs = 3.0\n",
        "            self.warmup_proportion = 0.1\n",
        "            self.no_cuda = False\n",
        "            self.seed = 11111\n",
        "            self.gradient_accumulation_steps = 1\n",
        "    return Args()\n",
        "\n",
        "args = get_args()\n",
        "\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "\n",
        "def load_audio_data(batch_dir, num_batches):\n",
        "    audio_data = []\n",
        "    for i in range(num_batches):\n",
        "        batch_path = f'{batch_dir}/batch_{i}.npy'\n",
        "        batch_data = np.load(batch_path)\n",
        "        audio_data.append(batch_data)\n",
        "    return np.concatenate(audio_data, axis=0)\n",
        "\n",
        "# Define batch directory and number of batches\n",
        "batch_dir = '/content/gdrive/MyDrive/BOAZ/mini_pj/all_mfcc_npy'\n",
        "num_batches = 39\n",
        "\n",
        "# Load audio data\n",
        "audio_npy = load_audio_data(batch_dir, num_batches)\n",
        "train_audio, test_audio = audio_npy[:15499], audio_npy[15499:]\n"
      ],
      "metadata": {
        "id": "vYwHoLw3LHvW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# InputExample 클래스 정의\n",
        "class InputExample(object):\n",
        "    def __init__(self, guid, text_a=None, text_b=None, label=None):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "# InputFeatures 클래스 정의\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "# convert_examples_to_features 함수 수정\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        tokens_b = tokenizer.tokenize(example.text_b) if example.text_b else []\n",
        "\n",
        "        if len(tokens_a) + len(tokens_b) > max_seq_length - 3:\n",
        "            tokens_a = tokens_a[:(max_seq_length - len(tokens_b) - 3)]\n",
        "\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        input_ids += [0] * padding_length\n",
        "        input_mask += [0] * padding_length\n",
        "        segment_ids += [0] * padding_length\n",
        "\n",
        "        label_id = label_map.get(example.label, 0)  # Ensure label_id is properly mapped\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                segment_ids=segment_ids,\n",
        "                label_id=label_id\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "enQetZQhLJay"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로딩 함수\n",
        "def load_data(args):\n",
        "    # Define paths to your data files\n",
        "    train_data_path = os.path.join(args.data_dir, \"/content/gdrive/MyDrive/BOAZ/mini_pj/train.tsv\")\n",
        "    test_data_path = os.path.join(args.data_dir, \"/content/gdrive/MyDrive/BOAZ/mini_pj/test.tsv\")\n",
        "\n",
        "    if not os.path.exists(train_data_path) or not os.path.exists(test_data_path):\n",
        "        raise FileNotFoundError(\"Train or test data file not found.\")\n",
        "\n",
        "    # Load train and test data\n",
        "    train_df = pd.read_csv(train_data_path, delimiter='\\t')\n",
        "    test_df = pd.read_csv(test_data_path, delimiter='\\t')\n",
        "\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "id": "vUiPTolRSOp9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import os\n",
        "\n",
        "def setup_model_and_data(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(f\"device: {device} n_gpu: {n_gpu}\")\n",
        "\n",
        "    if args.gradient_accumulation_steps < 1:\n",
        "        raise ValueError(f\"Invalid gradient_accumulation_steps parameter: {args.gradient_accumulation_steps}, should be >= 1\")\n",
        "\n",
        "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
        "\n",
        "    seed_num = np.random.randint(1, 10000)\n",
        "    random.seed(seed_num)\n",
        "    np.random.seed(seed_num)\n",
        "    torch.manual_seed(seed_num)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed_num)\n",
        "\n",
        "    if not args.do_train and not args.do_test:\n",
        "        raise ValueError(\"At least one of `do_train` or `do_test` must be True.\")\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    task_name = args.task_name.lower()\n",
        "\n",
        "    processors = {\n",
        "        \"multi\": PgProcessor,\n",
        "    }\n",
        "\n",
        "    num_labels_task = {\n",
        "        \"multi\": 7,\n",
        "    }\n",
        "\n",
        "    if task_name not in processors:\n",
        "        raise ValueError(f\"Task not found: {task_name}\")\n",
        "\n",
        "    processor = processors[task_name]()\n",
        "    num_labels = num_labels_task[task_name]\n",
        "    label_list = processor.get_labels()\n",
        "\n",
        "    tokenizer = KoBERTTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
        "\n",
        "    # Define cache_dir explicitly\n",
        "    cache_dir = args.cache_dir if args.cache_dir else './cache'\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(args.bert_model, cache_dir=cache_dir, num_labels=num_labels)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad = False\n",
        "        if \"encoder.layer.0\" in name or \"encoder.layer.1\" in name:\n",
        "            param.requires_grad = True\n",
        "        if \"encoder.layer.2\" in name or \"encoder.layer.3\" in name:\n",
        "            param.requires_grad = True\n",
        "        if \"encoder.layer.4\" in name or \"encoder.layer.5\" in name:\n",
        "            param.requires_grad = True\n",
        "        if \"encoder.layer.6\" in name or \"encoder.layer.7\" in name:\n",
        "            param.requires_grad = True\n",
        "        if \"encoder.layer.8\" in name or \"encoder.layer.9\" in name:\n",
        "            param.requires_grad = True\n",
        "        if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "            param.requires_grad = True\n",
        "        if \"BertFinetun\" in name or \"pooler\" in name:\n",
        "            param.requires_grad = True\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    if n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    return model, tokenizer, processor, num_labels, label_list, device\n",
        "# 모델 학습 함수\n",
        "def train_model(args, model, tokenizer, processor, device, train_audio):\n",
        "    if args.do_train:\n",
        "        train_examples = processor.get_train_examples(args.data_dir)\n",
        "        num_train_optimization_steps = int(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
        "\n",
        "        train_features = convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer)\n",
        "        print(\"***** Running training *****\")\n",
        "        print(\"  Num examples = %d\", len(train_examples))\n",
        "        print(\"  Batch size = %d\", args.train_batch_size)\n",
        "        print(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float32)\n",
        "        all_train_audio = torch.tensor(train_audio, dtype=torch.float32)\n",
        "\n",
        "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_train_audio, all_label_ids)\n",
        "        train_sampler = RandomSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "        # Define no_decay and new_decay parameters\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        new_decay = ['BertFine']\n",
        "\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and not any(np in n for np in new_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and any(np in n for np in new_decay)], 'lr': 0.01}\n",
        "        ]\n",
        "\n",
        "        # Use AdamW optimizer from transformers library\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False)\n",
        "\n",
        "        global_step = 0\n",
        "        tr_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, train_audio, label_ids = batch\n",
        "                model.zero_grad()\n",
        "                loss = model(input_ids, train_audio, segment_ids, input_mask, labels=label_ids)\n",
        "\n",
        "                if n_gpu > 1:\n",
        "                    loss = loss.mean()  # mean() to average on multi-g\n",
        "\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "        return model\n",
        "\n"
      ],
      "metadata": {
        "id": "gKpvbAwmLQcA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # 모델 및 데이터 설정 함수 호출\n",
        "    model, tokenizer, processor, num_labels, label_list, device, train_audio = setup_model_and_data(args)\n",
        "    print(\"Model and data setup successful!\")\n",
        "except Exception as e:\n",
        "    # 오류가 발생했을 때 오류 메시지 출력\n",
        "    print(f\"Error in setup_model_and_data: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MbPSvaKLbq3",
        "outputId": "7a77fa5a-de7f-4cdb-954a-e9a588789ce1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu n_gpu: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in setup_model_and_data: not enough values to unpack (expected 7, got 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 실행\n",
        "try:\n",
        "    trained_model = train_model(args, model, tokenizer, processor, device)\n",
        "    print(\"Model training successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97fbdQhrON20",
        "outputId": "8bc5e7ea-5e28-465c-b566-85b8756e03b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during training: name 'model' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, tokenizer, processor, device, eval_data_dir):\n",
        "    model.eval()\n",
        "    eval_examples = processor.get_dev_examples(eval_data_dir)\n",
        "    eval_features = convert_examples_to_features(eval_examples, label_list, args.max_seq_length, tokenizer)\n",
        "\n",
        "    all_input_ids = torch.tensor([f['input_ids'] for f in eval_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f['input_mask'] for f in eval_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f['segment_ids'] for f in eval_features], dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f['label_id'] for f in eval_features], dtype=torch.float32)\n",
        "\n",
        "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    eval_dataloader = DataLoader(eval_data, batch_size=args.eval_batch_size)\n",
        "\n",
        "    preds = []\n",
        "    labels = []\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, segment_ids, input_mask)\n",
        "            logits = outputs[0]\n",
        "        preds.append(logits.cpu().numpy())\n",
        "        labels.append(label_ids.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    accuracy = accuracy_score(labels, np.argmax(preds, axis=1))\n",
        "    f1 = f1_score(labels, np.argmax(preds, axis=1), average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# 모델 평가 실행\n",
        "evaluate_model(trained_model, tokenizer, processor, device, args.data_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "bXTE8ZS7MGXY",
        "outputId": "2e63a380-4925-445f-af20-44e3ac96208b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trained_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c2278c9e3e98>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# 모델 평가 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ml1iHXl7QPVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}